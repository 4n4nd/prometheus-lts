apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: prometheus
  annotations:
    "openshift.io/display-name": Prometheus
    description: |
      A monitoring solution for an OpenShift cluster - collect and gather metrics and alerts from nodes, services, and the infrastructure. This is a tech preview feature.
    iconClass: fa fa-cogs
    tags: "monitoring,prometheus, alertmanager,time-series"
parameters:
- description: The namespace to instantiate prometheus under. Defaults to 'kube-system'.
  name: NAMESPACE
  value: kube-system
- description: The location of the proxy image
  name: IMAGE_PROXY
  value: openshift/oauth-proxy:v1.0.0
- description: The location of the prometheus image
  name: IMAGE_PROMETHEUS
  value: openshift/prometheus:v2.2.1
- description: The location of the alertmanager image
  name: IMAGE_ALERTMANAGER
  value: openshift/prometheus-alertmanager:v0.13.0
- description: The location of alert-buffer image
  name: IMAGE_ALERT_BUFFER
  value: openshift/prometheus-alert-buffer:v0.0.2
- description: The session secret for the proxy
  name: SESSION_SECRET
  generate: expression
  from: "[a-zA-Z0-9]{43}"

objects:
# Authorize the prometheus service account to read data about the cluster
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
    annotations:
      serviceaccounts.openshift.io/oauth-redirectreference.prom: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"prometheus"}}'
      serviceaccounts.openshift.io/oauth-redirectreference.alerts: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"alerts"}}'
      serviceaccounts.openshift.io/oauth-redirectreference.alertmanager: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"alertmanager"}}'
- apiVersion: authorization.openshift.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: prometheus-cluster-reader
  roleRef:
    name: cluster-reader
  subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: "${NAMESPACE}"

# Create a fully end-to-end TLS connection to the prometheus proxy
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    to:
      name: prometheus
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/scheme: https
      service.alpha.openshift.io/serving-cert-secret-name: prometheus-tls
    labels:
      name: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: prometheus
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: prometheus
- apiVersion: v1
  kind: Secret
  metadata:
    name: prometheus-proxy
    namespace: "${NAMESPACE}"
  stringData:
    session_secret: "${SESSION_SECRET}="

# Create a fully end-to-end TLS connection to the alert proxy
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: alerts
    namespace: "${NAMESPACE}"
  spec:
    to:
      name: alerts
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      service.alpha.openshift.io/serving-cert-secret-name: alerts-tls
    labels:
      name: alerts
    name: alerts
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: alerts
      port: 443
      protocol: TCP
      targetPort: 9443
    selector:
      app: prometheus
- apiVersion: v1
  kind: Secret
  metadata:
    name: alerts-proxy
    namespace: "${NAMESPACE}"
  stringData:
    session_secret: "${SESSION_SECRET}="

# Create a fully end-to-end TLS connection to the alertmanager proxy
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    to:
      name: alertmanager
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      service.alpha.openshift.io/serving-cert-secret-name: alertmanager-tls
    labels:
      name: alertmanager
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: alertmanager
      port: 443
      protocol: TCP
      targetPort: 10443
    selector:
      app: prometheus
- apiVersion: v1
  kind: Secret
  metadata:
    name: alertmanager-proxy
    namespace: "${NAMESPACE}"
  stringData:
    session_secret: "${SESSION_SECRET}="

- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    labels:
      app: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    updateStrategy:
      type: RollingUpdate
    podManagementPolicy: Parallel
    selector:
      matchLabels:
        app: prometheus
    template:
      metadata:
        labels:
          app: prometheus
        name: prometheus
      spec:
        serviceAccountName: prometheus
        containers:
        # Deploy Prometheus behind an oauth proxy
        - name: prom-proxy
          image: ${IMAGE_PROXY}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 8443
            name: web
          args:
          - -provider=openshift
          - -https-address=:8443
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9090
          - -client-id=system:serviceaccount:${NAMESPACE}:prometheus
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - '-openshift-sar={"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -skip-auth-regex=^/metrics
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-tls-secret
          - mountPath: /etc/proxy/secrets
            name: prometheus-proxy-secret
          - mountPath: /prometheus
            name: prometheus-data

        - name: prometheus
          args:
          - --storage.tsdb.retention=6h
          - --config.file=/etc/prometheus/prometheus.yml
          - --web.listen-address=localhost:9090
          image: ${IMAGE_PROMETHEUS}
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: '1'
              memory: 4Gi
            requests:
              cpu: '1'
              memory: 4Gi
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
          - mountPath: /prometheus
            name: prometheus-data

        # Deploy alertmanager behind prometheus-alert-buffer behind an oauth proxy
        # use http port=4190 and https port=9943 to differ from prom-proxy
        - name: alerts-proxy
          image: ${IMAGE_PROXY}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 9443
            name: web
          args:
          - -provider=openshift
          - -https-address=:9443
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9099
          - -client-id=system:serviceaccount:${NAMESPACE}:prometheus
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - '-openshift-sar={"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          volumeMounts:
          - mountPath: /etc/tls/private
            name: alerts-tls-secret
          - mountPath: /etc/proxy/secrets
            name: alerts-proxy-secrets

        - name: alert-buffer
          args:
          - --storage-path=/alert-buffer/messages.db
          image: ${IMAGE_ALERT_BUFFER}
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - mountPath: /alert-buffer
            name: alerts-data
          ports:
          - containerPort: 9099
            name: alert-buf

        - name: alertmanager-proxy
          image: ${IMAGE_PROXY}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 10443
            name: web
          args:
          - -provider=openshift
          - -https-address=:10443
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9093
          - -client-id=system:serviceaccount:${NAMESPACE}:prometheus
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - '-openshift-sar={"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -skip-auth-regex=^/metrics
          volumeMounts:
          - mountPath: /etc/tls/private
            name: alertmanager-tls-secret
          - mountPath: /etc/proxy/secrets
            name: alertmanager-proxy-secret

        - name: alertmanager
          args:
          - --config.file=/etc/alertmanager/alertmanager.yml
          image: ${IMAGE_ALERTMANAGER}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 9093
            name: web
          volumeMounts:
          - mountPath: /etc/alertmanager
            name: alertmanager-config
          - mountPath: /alertmanager
            name: alertmanager-data

        restartPolicy: Always
        volumes:

        - name: prometheus-config
          configMap:
            defaultMode: 420
            name: prometheus
        - name: prometheus-proxy-secret
          secret:
            secretName: prometheus-proxy
        - name: prometheus-tls-secret
          secret:
            secretName: prometheus-tls
        - name: prometheus-data
          emptyDir: {}

        - name: alertmanager-config
          configMap:
            defaultMode: 420
            name: alertmanager
        - name: alertmanager-tls-secret
          secret:
            secretName: alertmanager-tls  
        - name: alertmanager-proxy-secret
          secret:
            secretName: alertmanager-proxy         

        - name: alerts-proxy-secrets
          secret:
            secretName: alerts-proxy
        - name: alerts-tls-secret
          secret:
            secretName: alerts-tls
        - name: alertmanager-data
          emptyDir: {}
        - name: alerts-data
          emptyDir: {}

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
  data:
    alerting.rules: |
      groups:
      - name: example-rules
        interval: 30s # defaults to global interval
        rules:
        - alert: Node Down
          expr: up{job="kubernetes-nodes"} == 0
          annotations:
            miqTarget: "ContainerNode"
            severity: "HIGH"
            message: "{{$labels.instance}} is down"
    
    recording.rules: |
      groups:
      - name: aggregate_container_resources
        rules:
        - record: container_cpu_usage_rate
          expr: sum without (cpu) (rate(container_cpu_usage_seconds_total[5m]))
        - record: container_memory_rss_by_type
          expr: container_memory_rss{id=~"/|/system.slice|/kubepods.slice"} > 0
        - record: container_cpu_usage_percent_by_host
          expr: sum by (hostname,type)(rate(container_cpu_usage_seconds_total{id="/"}[5m])) / on (hostname,type) machine_cpu_cores
        - record: apiserver_request_count_rate_by_resources
          expr: sum without (client,instance,contentType) (rate(apiserver_request_count[5m]))

    prometheus.yml: |
      rule_files:
        - '*.rules'
      scrape_configs:
      - job_name: 'prometheus'
        scrape_interval: 1m
        static_configs:
          - targets: ['localhost:9090']
      remote_write:
        - url: "http://influxdb:8086/api/v1/prom/write?u=admin&p={INFLUX_ADMIN_PASSWORD}&db=prometheus"
      remote_read:
        - url: "http://influxdb:8086/api/v1/prom/read?u=admin&p={INFLUX_ADMIN_PASSWORD}&db=prometheus"
      alerting:
        alertmanagers:
        - scheme: http
          static_configs:
          - targets:
            - "localhost:9093"

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: alertmanager
    namespace: "${NAMESPACE}"
  data:
    alertmanager.yml: |
      global:

      # The root route on which each incoming alert enters.
      route:
        # default route if none match
        receiver: alert-buffer-wh

        # The labels by which incoming alerts are grouped together. For example,
        # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
        # be batched into a single group.
        # TODO:
        group_by: []

        # All the above attributes are inherited by all child routes and can
        # overwritten on each.

      receivers:
      - name: alert-buffer-wh
        webhook_configs:
        - url: http://localhost:9099/topics/alerts
